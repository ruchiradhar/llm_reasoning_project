# ğŸ§  LLM Reasoning Benchmark

A comprehensive benchmark suite for evaluating small language models on reasoning tasks. This project tests multiple LLMs from HuggingFace on mathematical and logical reasoning challenges, generating comparative leaderboards to assess their capabilities.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ¯ Project Overview

This project demonstrates:
- **Model Loading & Management**: Efficient loading of HuggingFace transformer models
- **Task-Based Evaluation**: Structured evaluation framework for reasoning tasks
- **Automated Scoring**: Intelligent answer extraction and scoring algorithms
- **Results Visualization**: Clean leaderboard generation and results tracking
- **Production-Ready Code**: Modular architecture with proper logging and error handling

## ğŸ“Š Features

- âœ… **Multiple Model Support**: Evaluate various small LLMs (< 1M parameters)
- âœ… **Dual Task Evaluation**: 
  - Math Reasoning (10 questions)
  - Logic Reasoning (10 questions)
- âœ… **Automated Leaderboard**: Ranked comparison of model performance
- âœ… **Result Persistence**: JSON and CSV output formats
- âœ… **CLI Interface**: Easy-to-use command-line interface
- âœ… **Extensible Architecture**: Simple to add new tasks or models

## ğŸ—ï¸ Project Structure

```
llm_reasoning_project/
â”œâ”€â”€ benchmark.py              # Main benchmark script
â”œâ”€â”€ requirements.txt          # Project dependencies
â”œâ”€â”€ setup.py                 # Package setup configuration
â”œâ”€â”€ README.md                # Project documentation
â”œâ”€â”€ LICENSE                  # MIT license
â”œâ”€â”€ .gitignore              # Git ignore rules
â”‚
â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_loader.py      # HuggingFace model loader
â”‚   â””â”€â”€ evaluator.py         # Evaluation and leaderboard generation
â”‚
â”œâ”€â”€ tasks/                   # Task implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ math_reasoning.py    # Math reasoning tasks
â”‚   â””â”€â”€ logic_reasoning.py   # Logic reasoning tasks
â”‚
â””â”€â”€ results/                 # Output directory (generated)
    â”œâ”€â”€ results_<timestamp>.json
    â”œâ”€â”€ leaderboard_<timestamp>.csv
    â””â”€â”€ leaderboard_latest.csv
```

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd llm_reasoning_project

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Running the Benchmark

```bash
# Run benchmark on all default models
python benchmark.py

# Run on specific models
python benchmark.py --models sshleifer/tiny-gpt2 hf-internal-testing/tiny-random-gpt2

# List available models
python benchmark.py --list-models

# Run on GPU (if available)
python benchmark.py --device cuda

# Run without saving results
python benchmark.py --no-save
```

## ğŸ“‹ Available Models

The benchmark includes several tiny models suitable for testing:

- `sshleifer/tiny-gpt2` - Minimal GPT-2 variant (~10-50K parameters)
- `hf-internal-testing/tiny-random-gpt2` - Random initialized tiny GPT-2
- `sshleifer/tiny-ctrl` - Tiny CTRL model variant
- `distilgpt2` - Distilled GPT-2 (~82M parameters)

**Note**: Finding models under 1M parameters is challenging as most production LLMs are much larger. The included models represent the smallest available variants for demonstration purposes.

## ğŸ§ª Task Details

### Math Reasoning (10 Questions)
Tests basic arithmetic and mathematical problem-solving:
- Addition, subtraction, multiplication, division
- Word problems
- Area calculations
- Speed/distance problems

**Example**: "If you have 3 apples and buy 5 more, how many apples do you have in total?"

### Logic Reasoning (10 Questions)
Tests logical thinking, deduction, and pattern recognition:
- Syllogistic reasoning
- Pattern completion
- True/false logic evaluation
- Temporal reasoning

**Example**: "If A is taller than B, and B is taller than C, is A taller than C?"

## ğŸ“ˆ Evaluation Metrics

Each model receives:
- **Math Score**: Percentage correct on math tasks (0-100%)
- **Logic Score**: Percentage correct on logic tasks (0-100%)
- **Overall Score**: Average of math and logic scores

Results are ranked in a leaderboard format showing comparative performance.

## ğŸ¨ Example Output

```
================================================================================
                  ğŸ† LLM REASONING BENCHMARK LEADERBOARD ğŸ†                     
================================================================================

â•”â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•—
â•‘ Rank â•‘ Model                             â•‘ Overall  â•‘ Math    â•‘ Logic   â•‘
â• â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•£
â•‘    1 â•‘ sshleifer/tiny-gpt2              â•‘ 35.00%   â•‘ 40.00%  â•‘ 30.00%  â•‘
â•‘    2 â•‘ hf-internal-testing/tiny-random-  â•‘ 25.00%   â•‘ 20.00%  â•‘ 30.00%  â•‘
â•‘      â•‘ gpt2                              â•‘          â•‘         â•‘         â•‘
â•šâ•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•
```

## ğŸ”§ Technical Implementation

### Key Components

1. **ModelLoader** (`src/model_loader.py`)
   - Loads models from HuggingFace Hub
   - Manages tokenization and text generation
   - Provides parameter counting and model info

2. **Task Evaluators** (`tasks/`)
   - `MathReasoningTask`: Math problem evaluation
   - `LogicReasoningTask`: Logic problem evaluation
   - Answer extraction using regex and heuristics
   - Automated scoring with detailed results

3. **Evaluator** (`src/evaluator.py`)
   - Aggregates results across models and tasks
   - Generates ranked leaderboards
   - Exports results to JSON and CSV formats

4. **Benchmark Script** (`benchmark.py`)
   - CLI interface for running evaluations
   - Orchestrates model loading and task execution
   - Displays results and saves outputs

## ğŸ“¦ Dependencies

- **torch**: PyTorch for model execution
- **transformers**: HuggingFace transformers library
- **huggingface-hub**: HuggingFace Hub integration
- **pandas**: Data manipulation and analysis
- **tabulate**: Pretty table formatting
- **numpy**: Numerical operations
- **tqdm**: Progress bars

## ğŸ¤ Contributing

Contributions are welcome! Areas for enhancement:

- Add more reasoning tasks (coding, common sense, etc.)
- Support for larger model variants
- Implement caching for faster re-evaluation
- Add confidence scoring for answers
- Create web interface for results
- Add more sophisticated answer extraction

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¤ Author

Created as a demonstration of LLM evaluation capabilities and software engineering best practices.

## ğŸ™ Acknowledgments

- HuggingFace for providing model infrastructure
- The open-source ML community for model development
- PyTorch and Transformers teams for excellent libraries

## ğŸ¯ Skills Demonstrated

This project showcases:
- Machine Learning model integration and evaluation
- Python software architecture and design patterns
- Task automation and benchmark frameworks
- Data analysis and visualization
- CLI tool development
- Comprehensive documentation
- Git version control and professional project organization
A project to compare to setup and compare common LLMs on reasoning tasks
